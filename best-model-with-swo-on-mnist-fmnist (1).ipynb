{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#distant connection sparse with local receptive fields (FIXED)\nimport numpy as np\nimport cupy as cp\nimport time\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.metrics import accuracy_score\nfrom itertools import product\n\ncp.random.seed(41)\nsigmoid = lambda z: 1. / (1. + cp.exp(-z))\n\ndef orth_rows(m, n):\n    if m <= n:\n        q, _ = cp.linalg.qr(cp.random.randn(n, n))\n        return q[:m]\n    else:\n        q, _ = cp.linalg.qr(cp.random.randn(m, n))\n        return q\n\ndef random_patch_mask(n_hidden, input_dim, patch_size=(10, 10)):\n    \"\"\"Create mask with local receptive fields for first hidden layer\"\"\"\n    mask = cp.zeros((n_hidden, input_dim))\n    img_size = int(np.sqrt(input_dim))  # Convert to standard Python int\n    \n    for i in range(n_hidden):\n        # Get Python integers for coordinates\n        h_start = cp.random.randint(0, img_size - patch_size[0] + 1).item()\n        w_start = cp.random.randint(0, img_size - patch_size[1] + 1).item()\n        \n        # Create patch indices\n        patch_indices = []\n        for h in range(h_start, h_start + patch_size[0]):\n            for w in range(w_start, w_start + patch_size[1]):\n                idx = h * img_size + w\n                patch_indices.append(int(idx))  # Ensure integer index\n                \n        mask[i, patch_indices] = 1\n    return mask\n\ndef small_world_mask(n_input, n_hidden, k, r, p):\n    mask = cp.zeros((n_hidden, n_input))\n    for i in range(n_hidden):\n        center = cp.random.randint(0, n_input).item()  # Convert to Python int\n        neighbors = [(center + offset) % n_input for offset in range(-r, r+1)]\n        selected = neighbors if k > len(neighbors) else cp.random.choice(neighbors, size=k, replace=False)\n        mask[i, selected] = 1\n        for j, idx in enumerate(selected):\n            if cp.random.rand() < p:\n                mask[i, idx] = 0\n                new_idx = cp.random.randint(0, n_input).item()  # Convert to Python int\n                mask[i, new_idx] = 1\n    return mask\n\ndef train_layer(X_prev, n_hid, C, mask):\n    d_prev, N = X_prev.shape\n    W_raw = cp.random.randn(n_hid, d_prev)\n    W_in = W_raw * mask\n    b_in = cp.random.randn(n_hid, 1)\n\n    Z = W_in @ X_prev + b_in\n    H = sigmoid(Z)\n\n    HHT = H @ H.T\n    I = cp.eye(H.shape[0])\n    W_out = cp.linalg.solve(I / C + HHT, H @ X_prev.T)\n    return H, (W_in, b_in)\n\ndef ml_elm_mnist(k, r, p, p_dist):\n    struct = (784, 700, 700, 5000, 10)\n    Cs = (1e-1, 1e3, 1e8, 1e8)\n\n    (x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n    x_tr = cp.asarray(x_tr.reshape(-1, 28*28).T / 255.)\n    x_te = cp.asarray(x_te.reshape(-1, 28*28).T / 255.)\n    Y_train = cp.asarray((np.eye(10)[y_tr].T) * 2 - 1)\n\n    # H0 = input layer\n    H0 = x_tr\n\n    # Hidden Layer 1 with random patch sampling\n    mask1 = random_patch_mask(struct[1], struct[0])  # 10x10 patches\n    H1, params1 = train_layer(H0, struct[1], Cs[0], mask1)\n\n    # Hidden Layer 2\n    mask2 = small_world_mask(struct[1], struct[2], k, r, p)\n    H2, params2 = train_layer(H1, struct[2], Cs[1], mask2)\n\n    # Hidden Layer 3 with sparse H1-to-H3 distant connections\n    H_concat = cp.concatenate((H2, H1), axis=0)\n    mask3 = small_world_mask(H_concat.shape[0], struct[3], k, r, p)\n\n    # Apply sparsity to H1 part of H_concat using p_dist\n    h1_start = H2.shape[0]\n    mask3[:, h1_start:] *= (cp.random.rand(struct[3], H1.shape[0]) > p_dist)\n\n    H3, params3 = train_layer(H_concat, struct[3], Cs[2], mask3)\n\n    # Output layer\n    HHT = H3 @ H3.T\n    I = cp.eye(H3.shape[0])\n    W_out = cp.linalg.solve(I / Cs[-1] + HHT, H3 @ Y_train.T)\n\n    def forward(X):\n        H0 = X\n        Z1 = params1[0] @ H0 + params1[1]\n        H1 = sigmoid(Z1)\n\n        Z2 = params2[0] @ H1 + params2[1]\n        H2 = sigmoid(Z2)\n\n        H_concat = cp.concatenate((H2, H1), axis=0)\n        Z3 = params3[0] @ H_concat + params3[1]\n        H3 = sigmoid(Z3)\n\n        return W_out.T @ H3\n\n    tr_pred = cp.asnumpy(forward(x_tr)).argmax(0)\n    te_pred = cp.asnumpy(forward(x_te)).argmax(0)\n    tr_acc = accuracy_score(y_tr, tr_pred)\n    te_acc = accuracy_score(y_te, te_pred)\n    return tr_acc, te_acc\n\n# Hyperparameters\nk_list = [4]\nr_list = [4]\np_list = [0.02]\np_dist_list = [0.05]  # probability of dropping H1→H3 connections\n\nparam_grid = list(product(k_list, r_list, p_list, p_dist_list))\n\nprint(f\"Total Runs: {len(param_grid)}\")\nbest_acc = 0\nbest_params = None\nstart_total = time.time()\n\nfor i, (k, r, p, p_dist) in enumerate(param_grid):\n    print(f\"Trial {i+1}/{len(param_grid)}: k={k}, r={r}, p={p:.2f}, p_dist={p_dist:.2f}\")\n    tr_acc, te_acc = ml_elm_mnist(k, r, p, p_dist)\n    print(f\"Train Acc: {tr_acc*100:.2f}%, Test Acc: {te_acc*100:.2f}%\")\n    if te_acc > best_acc:\n        best_acc = te_acc\n        best_params = (k, r, p, p_dist)\n    print(f\"Best So Far: {best_acc*100:.2f}% with k={best_params[0]}, r={best_params[1]}, p={best_params[2]}, p_dist={best_params[3]}\")\n\nprint(\"\\n==== Grid Search Completed ====\")\nprint(f\"Best Parameters: k={best_params[0]}, r={best_params[1]}, p={best_params[2]}, p_dist={best_params[3]}\")\nprint(f\"Best Test Accuracy: {best_acc*100:.2f}%\")\nprint(f\"Total Time: {(time.time()-start_total)/60:.2f} min\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:05:50.297883Z","iopub.execute_input":"2025-07-04T13:05:50.298524Z","iopub.status.idle":"2025-07-04T13:06:41.446561Z","shell.execute_reply.started":"2025-07-04T13:05:50.298505Z","shell.execute_reply":"2025-07-04T13:06:41.445890Z"}},"outputs":[{"name":"stdout","text":"Total Runs: 1\nTrial 1/1: k=4, r=4, p=0.02, p_dist=0.05\nTrain Acc: 98.43%, Test Acc: 97.53%\nBest So Far: 97.53% with k=4, r=4, p=0.02, p_dist=0.05\n\n==== Grid Search Completed ====\nBest Parameters: k=4, r=4, p=0.02, p_dist=0.05\nBest Test Accuracy: 97.53%\nTotal Time: 0.85 min\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Full MELM + SWOA (Single Cell) ===\nimport numpy as np\nimport cupy as cp\nimport time\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.metrics import accuracy_score\n\ncp.random.seed(41)\nsigmoid = lambda z: 1. / (1. + cp.exp(-z))\n\ndef orth_rows(m, n):\n    if m <= n:\n        q, _ = cp.linalg.qr(cp.random.randn(n, n))\n        return q[:m]\n    else:\n        q, _ = cp.linalg.qr(cp.random.randn(m, n))\n        return q\n\ndef random_patch_mask(n_hidden, input_dim, patch_size=(10, 10)):\n    mask = cp.zeros((n_hidden, input_dim))\n    img_size = int(np.sqrt(input_dim))\n    for i in range(n_hidden):\n        h_start = cp.random.randint(0, img_size - patch_size[0] + 1).item()\n        w_start = cp.random.randint(0, img_size - patch_size[1] + 1).item()\n        patch_indices = []\n        for h in range(h_start, h_start + patch_size[0]):\n            for w in range(w_start, w_start + patch_size[1]):\n                idx = h * img_size + w\n                patch_indices.append(int(idx))\n        mask[i, patch_indices] = 1\n    return mask\n\ndef small_world_mask(n_input, n_hidden, k, r, p):\n    mask = cp.zeros((n_hidden, n_input))\n    for i in range(n_hidden):\n        center = cp.random.randint(0, n_input).item()\n        neighbors = [(center + offset) % n_input for offset in range(-r, r+1)]\n        selected = neighbors if k > len(neighbors) else cp.random.choice(neighbors, size=k, replace=False)\n        mask[i, selected] = 1\n        for j, idx in enumerate(selected):\n            if cp.random.rand() < p:\n                mask[i, idx] = 0\n                new_idx = cp.random.randint(0, n_input).item()\n                mask[i, new_idx] = 1\n    return mask\n\ndef train_layer(X_prev, n_hid, C, mask):\n    d_prev, N = X_prev.shape\n    W_raw = cp.random.randn(n_hid, d_prev)\n    W_in = W_raw * mask\n    b_in = cp.random.randn(n_hid, 1)\n    Z = W_in @ X_prev + b_in\n    H = sigmoid(Z)\n    HHT = H @ H.T\n    I = cp.eye(H.shape[0])\n    W_out = cp.linalg.solve(I / C + HHT, H @ X_prev.T)\n    return H, (W_in, b_in)\n\ndef ml_elm_mnist(k, r, p, p_dist):\n    struct = (784, 700, 700, 5000, 10)\n    Cs = (1e-1, 1e3, 1e8, 1e8)\n    (x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n    x_tr = cp.asarray(x_tr.reshape(-1, 28*28).T / 255.)\n    x_te = cp.asarray(x_te.reshape(-1, 28*28).T / 255.)\n    Y_train = cp.asarray((np.eye(10)[y_tr].T) * 2 - 1)\n    H0 = x_tr\n    mask1 = random_patch_mask(struct[1], struct[0])\n    H1, params1 = train_layer(H0, struct[1], Cs[0], mask1)\n    mask2 = small_world_mask(struct[1], struct[2], k, r, p)\n    H2, params2 = train_layer(H1, struct[2], Cs[1], mask2)\n    H_concat = cp.concatenate((H2, H1), axis=0)\n    mask3 = small_world_mask(H_concat.shape[0], struct[3], k, r, p)\n    h1_start = H2.shape[0]\n    mask3[:, h1_start:] *= (cp.random.rand(struct[3], H1.shape[0]) > p_dist)\n    H3, params3 = train_layer(H_concat, struct[3], Cs[2], mask3)\n    HHT = H3 @ H3.T\n    I = cp.eye(H3.shape[0])\n    W_out = cp.linalg.solve(I / Cs[-1] + HHT, H3 @ Y_train.T)\n\n    def forward(X):\n        H0 = X\n        Z1 = params1[0] @ H0 + params1[1]\n        H1 = sigmoid(Z1)\n        Z2 = params2[0] @ H1 + params2[1]\n        H2 = sigmoid(Z2)\n        H_concat = cp.concatenate((H2, H1), axis=0)\n        Z3 = params3[0] @ H_concat + params3[1]\n        H3 = sigmoid(Z3)\n        return W_out.T @ H3\n\n    tr_pred = cp.asnumpy(forward(x_tr)).argmax(0)\n    te_pred = cp.asnumpy(forward(x_te)).argmax(0)\n    tr_acc = accuracy_score(y_tr, tr_pred)\n    te_acc = accuracy_score(y_te, te_pred)\n    return tr_acc, te_acc\n\n# === SWOA ===\nPOP_SIZE = 6\nMAX_ITERS = 10\nmutation_prob = 0.7\nk_bounds = (2, 8)\nr_bounds = (1, 6)\np_bounds = (0.01, 0.1)\np_dist_bounds = (0.01, 0.1)\n\ndef random_candidate():\n    return [\n        np.random.randint(*k_bounds),\n        np.random.randint(*r_bounds),\n        np.random.uniform(*p_bounds),\n        np.random.uniform(*p_dist_bounds)\n    ]\n\ndef clip_params(s):\n    s[0] = int(np.clip(s[0], *k_bounds))\n    s[1] = int(np.clip(s[1], *r_bounds))\n    s[2] = float(np.clip(s[2], *p_bounds))\n    s[3] = float(np.clip(s[3], *p_dist_bounds))\n    return s\n\ndef fitness(s):\n    try:\n        return ml_elm_mnist(*s)[1]\n    except Exception as e:\n        print(f\"Error for {s}: {e}\")\n        return 0.0\n\ndef local_shortcut(s):\n    s_new = s.copy()\n    idx = np.random.randint(4)\n    if idx in [0, 1]:\n        s_new[idx] += np.random.choice([-1, 1])\n    else:\n        s_new[idx] += np.random.uniform(-0.01, 0.01)\n    return clip_params(s_new)\n\ndef random_long_jump():\n    return random_candidate()\n\npopulation = [random_candidate() for _ in range(POP_SIZE)]\nfitness_vals = [fitness(s) for s in population]\nbest_idx = np.argmax(fitness_vals)\nbest_sol = population[best_idx]\nbest_fit = fitness_vals[best_idx]\n\nstart_time = time.time()\n\nfor it in range(MAX_ITERS):\n    new_population = []\n    for i in range(POP_SIZE):\n        s_base = population[i]\n        s_prime = random_long_jump() if np.random.rand() > mutation_prob else s_base\n        s_new = local_shortcut(s_prime)\n        f_old = fitness_vals[i]\n        f_new = fitness(s_new)\n        if f_new > f_old:\n            new_population.append(s_new)\n            fitness_vals[i] = f_new\n        else:\n            new_population.append(s_base)\n        if f_new > best_fit:\n            best_fit = f_new\n            best_sol = s_new\n            print(f\"[Iter {it+1}] New Best: {best_fit*100:.2f}% with k={best_sol[0]}, r={best_sol[1]}, p={best_sol[2]:.3f}, p_dist={best_sol[3]:.3f}\")\n    population = new_population\n    if best_fit > 0.98:\n        break\n\nend_time = time.time()\nprint(\"\\n==== SWOA Completed ====\")\nprint(f\"Best Params: k={best_sol[0]}, r={best_sol[1]}, p={best_sol[2]:.3f}, p_dist={best_sol[3]:.3f}\")\nprint(f\"Best Test Accuracy: {best_fit*100:.2f}%\")\nprint(f\"Total Time: {(end_time - start_time)/60:.2f} min\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T14:23:56.203822Z","iopub.execute_input":"2025-07-04T14:23:56.204050Z","iopub.status.idle":"2025-07-04T15:19:05.179424Z","shell.execute_reply.started":"2025-07-04T14:23:56.204030Z","shell.execute_reply":"2025-07-04T15:19:05.178609Z"}},"outputs":[{"name":"stderr","text":"2025-07-04 14:24:00.045134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751639040.232660      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751639040.288981      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n[Iter 5] New Best: 97.59% with k=6, r=2, p=0.098, p_dist=0.066\n[Iter 8] New Best: 97.60% with k=2, r=4, p=0.081, p_dist=0.024\n\n==== SWOA Completed ====\nBest Params: k=2, r=4, p=0.081, p_dist=0.024\nBest Test Accuracy: 97.60%\nTotal Time: 49.91 min\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport cupy as cp\nimport time\nfrom tensorflow.keras.datasets import mnist\nfrom sklearn.metrics import accuracy_score\n\ncp.random.seed(41)\nsigmoid = lambda z: 1. / (1. + cp.exp(-z))\n\ndef orth_rows(m, n):\n    if m <= n:\n        q, _ = cp.linalg.qr(cp.random.randn(n, n))\n        return q[:m]\n    else:\n        q, _ = cp.linalg.qr(cp.random.randn(m, n))\n        return q\n\ndef random_patch_mask(n_hidden, input_dim, patch_size=(10, 10)):\n    mask = cp.zeros((n_hidden, input_dim))\n    img_size = int(np.sqrt(input_dim))\n    for i in range(n_hidden):\n        h_start = cp.random.randint(0, img_size - patch_size[0] + 1).item()\n        w_start = cp.random.randint(0, img_size - patch_size[1] + 1).item()\n        patch_indices = []\n        for h in range(h_start, h_start + patch_size[0]):\n            for w in range(w_start, w_start + patch_size[1]):\n                idx = h * img_size + w\n                patch_indices.append(int(idx))\n        mask[i, patch_indices] = 1\n    return mask\n\ndef small_world_mask(n_input, n_hidden, k, r, p):\n    mask = cp.zeros((n_hidden, n_input))\n    for i in range(n_hidden):\n        center = cp.random.randint(0, n_input).item()\n        neighbors = [(center + offset) % n_input for offset in range(-r, r+1)]\n        selected = neighbors if k > len(neighbors) else cp.random.choice(neighbors, size=k, replace=False)\n        mask[i, selected] = 1\n        for idx in selected:\n            if cp.random.rand() < p:\n                mask[i, idx] = 0\n                new_idx = cp.random.randint(0, n_input).item()\n                mask[i, new_idx] = 1\n    return mask\n\ndef train_layer(X_prev, n_hid, C, mask, custom_weights=None, custom_bias=None):\n    d_prev, N = X_prev.shape\n    W_in = custom_weights if custom_weights is not None else cp.random.randn(n_hid, d_prev) * mask\n    b_in = custom_bias if custom_bias is not None else cp.random.randn(n_hid, 1)\n    Z = W_in @ X_prev + b_in\n    H = sigmoid(Z)\n    HHT = H @ H.T\n    I = cp.eye(H.shape[0])\n    W_out = cp.linalg.solve(I / C + HHT, H @ X_prev.T)\n    return H, (W_in, b_in)\n\ndef ml_elm_mnist_weights(k, r, p, p_dist, layer_weights=None):\n    struct = (784, 700, 700, 5000, 10)\n    Cs = (1e-1, 1e3, 1e8, 1e8)\n    (x_tr, y_tr), (x_te, y_te) = mnist.load_data()\n    x_tr = cp.asarray(x_tr.reshape(-1, 28*28).T / 255.)\n    x_te = cp.asarray(x_te.reshape(-1, 28*28).T / 255.)\n    Y_train = cp.asarray((np.eye(10)[y_tr].T) * 2 - 1)\n    H0 = x_tr\n\n    mask1 = random_patch_mask(struct[1], struct[0])\n    W1, B1 = None, None if layer_weights is None else layer_weights[0]\n    H1, (W1, B1) = train_layer(H0, struct[1], Cs[0], mask1, W1, B1)\n\n    mask2 = small_world_mask(struct[1], struct[2], k, r, p)\n    W2, B2 = None, None if layer_weights is None else layer_weights[1]\n    H2, (W2, B2) = train_layer(H1, struct[2], Cs[1], mask2, W2, B2)\n\n    H_concat = cp.concatenate((H2, H1), axis=0)\n    mask3 = small_world_mask(H_concat.shape[0], struct[3], k, r, p)\n    h1_start = H2.shape[0]\n    mask3[:, h1_start:] *= (cp.random.rand(struct[3], H1.shape[0]) > p_dist)\n    W3, B3 = None, None if layer_weights is None else layer_weights[2]\n    H3, (W3, B3) = train_layer(H_concat, struct[3], Cs[2], mask3, W3, B3)\n\n    HHT = H3 @ H3.T\n    I = cp.eye(H3.shape[0])\n    W_out = cp.linalg.solve(I / Cs[-1] + HHT, H3 @ Y_train.T)\n\n    def forward(X):\n        Z1 = W1 @ X + B1\n        H1 = sigmoid(Z1)\n        Z2 = W2 @ H1 + B2\n        H2 = sigmoid(Z2)\n        H_concat = cp.concatenate((H2, H1), axis=0)\n        Z3 = W3 @ H_concat + B3\n        H3 = sigmoid(Z3)\n        return W_out.T @ H3\n\n    tr_pred = cp.asnumpy(forward(x_tr)).argmax(0)\n    te_pred = cp.asnumpy(forward(x_te)).argmax(0)\n    tr_acc = accuracy_score(y_tr, tr_pred)\n    te_acc = accuracy_score(y_te, te_pred)\n    return tr_acc, te_acc, [(W1, B1), (W2, B2), (W3, B3)]\n\n# === Phase 1: SWOA Optimization for (k, r, p, p_dist) ===\nPOP_SIZE = 4\nMAX_ITERS_PHASE1 = 10\nmutation_prob = 0.7\nk_bounds = (2, 8)\nr_bounds = (1, 6)\np_bounds = (0.01, 0.1)\np_dist_bounds = (0.01, 0.1)\n\ndef random_candidate():\n    return [\n        np.random.randint(*k_bounds),\n        np.random.randint(*r_bounds),\n        np.random.uniform(*p_bounds),\n        np.random.uniform(*p_dist_bounds)\n    ]\n\ndef clip_params(s):\n    s[0] = int(np.clip(s[0], *k_bounds))\n    s[1] = int(np.clip(s[1], *r_bounds))\n    s[2] = float(np.clip(s[2], *p_bounds))\n    s[3] = float(np.clip(s[3], *p_dist_bounds))\n    return s\n\ndef fitness_phase1(s):\n    try:\n        _, acc, _ = ml_elm_mnist_weights(*s)\n        return acc\n    except:\n        return 0.0\n\ndef local_shortcut(s):\n    s_new = s.copy()\n    idx = np.random.randint(4)\n    s_new[idx] += np.random.choice([-1, 1]) if idx < 2 else np.random.uniform(-0.01, 0.01)\n    return clip_params(s_new)\n\ndef random_long_jump():\n    return random_candidate()\n\npopulation = [random_candidate() for _ in range(POP_SIZE)]\nfitness_vals = [fitness_phase1(s) for s in population]\nbest_idx = np.argmax(fitness_vals)\nbest_sol = population[best_idx]\nbest_fit = fitness_vals[best_idx]\n\nfor it in range(MAX_ITERS_PHASE1):\n    print(f\"[Phase 1] Iteration {it+1}/{MAX_ITERS_PHASE1}\")\n    new_population = []\n    for i in range(POP_SIZE):\n        s_base = population[i]\n        s_prime = random_long_jump() if np.random.rand() > mutation_prob else s_base\n        s_new = local_shortcut(s_prime)\n        f_old = fitness_vals[i]\n        f_new = fitness_phase1(s_new)\n        if f_new > f_old:\n            new_population.append(s_new)\n            fitness_vals[i] = f_new\n        else:\n            new_population.append(s_base)\n        if f_new > best_fit:\n            best_fit = f_new\n            best_sol = s_new\n            print(f\"[Phase 1][Iter {it+1}] New Best Acc: {best_fit*100:.2f}% -> k={s_new[0]}, r={s_new[1]}, p={s_new[2]:.3f}, p_dist={s_new[3]:.3f}\")\n    population = new_population\n\n# === Phase 2: Optimize Weights ===\nprint(\"\\n[Phase 2] Optimizing Weights using SWOA\")\n\nweights_base = ml_elm_mnist_weights(*best_sol)[2]\nweight_population = [weights_base for _ in range(POP_SIZE)]\n\ndef mutate_weights(weights):\n    new_weights = []\n    for W, B in weights:\n        W_mut = W + cp.random.normal(0, 0.01, W.shape)\n        B_mut = B + cp.random.normal(0, 0.01, B.shape)\n        new_weights.append((W_mut, B_mut))\n    return new_weights\n\ndef fitness_phase2(weights):\n    try:\n        _, acc, _ = ml_elm_mnist_weights(*best_sol, layer_weights=weights)\n        return acc\n    except:\n        return 0.0\n\nfitness_vals = [fitness_phase2(w) for w in weight_population]\nbest_idx = np.argmax(fitness_vals)\nbest_weights = weight_population[best_idx]\nbest_fit = fitness_vals[best_idx]\n\nfor it in range(MAX_ITERS_PHASE1):\n    print(f\"[Phase 2] Iteration {it+1}/{MAX_ITERS_PHASE1}\")\n    new_population = []\n    for i in range(POP_SIZE):\n        base_w = weight_population[i]\n        new_w = mutate_weights(base_w)\n        f_old = fitness_vals[i]\n        f_new = fitness_phase2(new_w)\n        if f_new > f_old:\n            new_population.append(new_w)\n            fitness_vals[i] = f_new\n        else:\n            new_population.append(base_w)\n        if f_new > best_fit:\n            best_fit = f_new\n            best_weights = new_w\n            print(f\"[Phase 2][Iter {it+1}] Improved Test Accuracy: {best_fit*100:.2f}%\")\n    weight_population = new_population\n\nprint(\"\\n==== Final Model Evaluation ====\")\nprint(f\"Best (k, r, p, p_dist): {best_sol}\")\nprint(f\"Best Final Test Accuracy: {best_fit*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T15:28:27.245129Z","iopub.execute_input":"2025-07-04T15:28:27.245405Z"}},"outputs":[{"name":"stderr","text":"2025-07-04 15:28:31.045544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751642911.205659      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751642911.251818      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n[Phase 1] Iteration 1/10\n[Phase 1][Iter 1] New Best Acc: 97.60% -> k=2, r=2, p=0.036, p_dist=0.028\n[Phase 1] Iteration 2/10\n[Phase 1] Iteration 3/10\n[Phase 1] Iteration 4/10\n[Phase 1] Iteration 5/10\n[Phase 1][Iter 5] New Best Acc: 97.61% -> k=2, r=2, p=0.036, p_dist=0.030\n[Phase 1] Iteration 6/10\n[Phase 1] Iteration 7/10\n[Phase 1] Iteration 8/10\n[Phase 1] Iteration 9/10\n","output_type":"stream"}],"execution_count":null}]}